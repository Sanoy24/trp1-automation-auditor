# --- LLM Provider Settings --------------------------------------------------
# Default provider and model to use for all nodes.
# Supported providers: ollama, openai, anthropic, google (gemini)
LLM_PROVIDER=ollama
LLM_MODEL=qwen2.5

# Base URL (Ollama only)
LLM_BASE_URL=http://localhost:11434

# --- Role-Specific Overrides (Optional) ------------------------------------
# You can use different LLMs for different parts of the swarm.
# If not set, they fall back to the default LLM_PROVIDER/MODEL above.

# Judges (Prosecutor, Defense, TechLead)
JUDGE_LLM_PROVIDER=ollama
JUDGE_LLM_MODEL=qwen2.5

# Chief Justice (Synthesis and Narrative)
JUSTICE_LLM_PROVIDER=ollama
JUSTICE_LLM_MODEL=qwen2.5

# Vision Inspector (Multimodal Diagram Analysis)
VISION_LLM_PROVIDER=ollama
VISION_LLM_MODEL=llava

# --- Provider API Keys -----------------------------------------------------
# Only required if using the respective provider.
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AIza...
GROQ_API_KEY=gsk_...

# --- LangSmith Observability -----------------------------------------------
# 1. Create an account at https://smith.langchain.com/
# 2. Set TRACING_V2 to true.
# 3. Paste your API Key below.
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=lsv2_pt_...
LANGCHAIN_PROJECT=your-project-name
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# --- Auditor Settings ------------------------------------------------------
# Local path to the rubric JSON specification.
RUBRIC_PATH=rubric/rubric.json

# Default directory for audit reports.
OUTPUT_DIR=audit/report_onself_generated